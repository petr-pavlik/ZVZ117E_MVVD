# Závislost dvou veličin {#seq-zavislost}

## Kovariance

Míru těsnosti vztahu dvou proměnných lze vajádřit kovariancí

$$
\mathrm{cov}_{xy} = \dfrac{1}{n-1}\sum_{i=1}^{n}x_iy_i
$$

Takový výsledek, produkt, není v praxi dobře interpretovatelný.

## Korelace {#sec-korelace}

Standardizovaný sdílený rozptyl dvou veličiny je korelace. Vyjadřuje, zda se jedna 
ze zkoumaných veličin mění, pokud se zároveň mění i druhá. Korelace neadresuje 
příčinnost, nelze tedy jednoznačně určit závislou a nezávislou proměnnou.

Míru korelace dvou veličin posuzujeme korelačním koeficientem. V případe normality 
u obou veličin lze použít **Paersonův** korelační koeficient,

$$
r_{xy} = \dfrac{\sum(x_i - \bar{x})(y_i - \bar{y})}{\sqrt{\sum(x_i - \bar{x})^2}\sqrt{\sum(y_i - \bar{y})^2}}
$$ {#eq-pearson}

přičemž platí $r_{xy}\in\langle-1;1\rangle$. Alternativně můžeme použít neparametrický výpočet **Spearmana**, založený na diferencích pořadí pozorovaných hodnot, které definujeme jako $d_i = x_{ri} - y_{ri}$

$$
\rho = r_s = 1 - \dfrac{6\sum_{i=1}^{n}d_i^2}{n(n^2-1)} 
$$ {#eq-spearman}

::: callout-tip
## Cvičení

1. Napište vlastní funkci, které pro dané vektory `x` a `y` spočítá korelační Spearmanův koeficient.\
  a) Nejprve je nutné identifikovat pořadí,\
  b) dále spočítat kvadrát vzdáleností,\
  c) a po dosazení počtu měření $n$ dosadit do rovnice.\
2. Vyzkoušejte obě funkce na datech

Řešení:\
```{r, eval=FALSE}
#| code-fold: true
sp_k <- function(x, y) {
  rank_x <- rank(x) 
  rank_y <- rank(y) 
  d <- rank_x - rank_y 
  d_sq <- sum(d^2) 
  n <- length(x) 
  1 - ((6 * d_sq) / (n*(n^2 - 1))) 
}
```


```{r, eval=FALSE}
sp_k(_, _)
cor(_, _, method = "Spearman")
```

3. Pokuste se odhadnout korelační koeficient následujících veličin

```{r, echo=FALSE, fig.align='center'}
par(mfrow = c(2, 2))
  x1 <- sin(seq(-1, 4, length.out = 100) + rnorm(100, sd = 0.4))
  x2 <- sort(c(sin(seq(-1, 4, length.out = 50)), 
                cos(seq(-3, 1, length.out = 50)))) + rnorm(n = 100, 
                                                           sd = 1.4)
  x3 <- 1:100/10
  y3 <- 5 + rnorm(100, sd = 5.5)
  x4 <- 1:100 * rnorm(100, sd = 500)
plot(x = x1, 
     main = "A)",
     xlab = "", 
     ylab = "", 
     axes = FALSE)
plot(x = x2, 
     main = "B)",
     xlab = "", 
     ylab = "", 
     axes = FALSE)
plot(x = x3, 
     y = y3, 
     main = "C)",
     xlab = "", 
     ylab = "", 
     axes = FALSE, 
     xlim = c(-10, 25), 
     ylim = c(-10, 20))
plot(x = x4, 
     xlab = "", 
     ylab = "", 
     main = "D)",
     axes = FALSE)
```

  s tolerancí 0.05. A) `r fitb(round(cor(1:100, x1), 2), num = TRUE, tol = 0.05)`, 
  B) `r fitb(round(cor(1:100, x2), 2), num = TRUE, tol = 0.05)`, 
  C) `r fitb(round(cor(x3, y3), 2), num = TRUE, tol = 0.05)`, 
  D) `r fitb(round(cor(1:100, x4), 2), num = TRUE, tol = 0.05)`. Pro desetinný rozvoj použijte tečku.
:::

## Regresní model

Narozdíl od korelace, v regresní analýze sledujeme příčinnou závislost. Identifikujeme **nezávislou proměnnou** a **závislou proměnnou**. Míru závislosti posuzujeme pomocí **lineárního modelu**. 

$$
y = \beta_0 + \beta_1x
$$ kde $y$ je závislá proměnná, $x$ je nezávislá proměnná, $\beta_0$ je počátek (intercept) a $\beta_1$ je sklon přímky.

Lineární model regresního typu definujeme v R pomocí funkce `lm()`, které na první pozici dosazujeme výraz ve formátu rovnice `formula`, např. `lm(y ~ x)`. Model pak vyhodnocujeme funkcí `summary()` a mezi sebou porovnáváme pomocí kritérií zohledňující vysvětlenou variabilitu/věrohodnost modelu a jeho komplexnost, např. **Akaikeho** - `AIC()` nebo **Bayesovské informační kritérium** `BIC()`.

$$
\text{AIC} = 2k - 2\ln(\hat{L})
$$ kde $k$ je počet volných parametrů modelu a $\hat{L}$ je věrohodnostní funkce modelu.

### První regresní model

  1. Uvažujme následující data, kde předpokládáme příčinnou závislost $y$ na $x$.

```{r}
dfr <- data.frame(
  x <- c( 3.93,  3.83,  7.11,  6.98,  7.87,  9.11, 10.04,  9.99, 10.11, 10.93, 
         11.01, 11.93, 11.94, 11.88, 11.82, 12.98, 13.04, 13.04, 13.11, 13.99, 
         13.97, 13.99, 13.97, 15.08, 14.93, 14.86, 15.89, 16.15, 17.08, 17.18, 
         16.96, 17.95, 18.14, 18.09, 18.05, 19.04, 19.08, 19.07, 20.11, 20, 
         20,    20.14, 19.92, 22.13, 23.09, 23.9 , 23.95, 24.06, 23.95, 24.94
  ),
  y <- c( 1.93, 10.01,  3.73, 22.09, 15.86, 10.09, 18.09, 25.91, 34.09, 16.89, 
         28.04, 13.95, 19.98, 23.98, 27.78, 26.19, 33.85, 33.96, 45.88, 25.96, 
         35.89, 60.  , 80.03, 20.06, 25.92, 54.03, 31.96, 40.12, 32.03, 39.95, 
         50.2 , 41.91, 55.92, 75.92, 83.89, 35.94, 45.94, 67.85, 32.05, 48.11, 
         52.11, 55.95, 64.02, 65.9 , 53.94, 69.92, 92.1 , 92.78, 99.03, 85.09)
)
```
  
  2. Sestrojíme lineární model funkcí `lm()` a uložíme do proměnné `md1`.

```{r}
md1 <- lm(formula = y ~ x, data = dfr)
```

  3. Struktura objektu `md1` je komplexní, můžeme vybírat jednotlivé proměnné k dalším účelům, například pro tvorbu grafů.

```{r}
str(md1, 1) #<1>
```
1. Vypíšeme strukturu objektu do první hierarchické úrovně


```{r, fig.align='center'}
with(data = dfr, #<1>
     expr = plot(x, #<1>
                 y, #<1>
                 xlim = c(0, 30), #<1>
                 ylim = c(md1$coefficients[1], #<1>
                          max(md1$fitted.values) + 2))) #<1>
abline(md1$coefficients, col = "orangered") #<2> 
abline(v = 0, lty = "dashed") #<3>
abline(h = md1$coefficients[1], lty = "dashed") #<3>
text(x = 5,
     y = md1$coefficients[1] + 4, 
     labels = expression(paste(beta == round(md1$coefficients[1], 2))))
```
1. Určíme základní proměnné $x$ a $y$ v grafu a nastavíme limity okna.
2. Z výstupu objektu lineárního modelu vytáhneme hodnoty koeficientů $\hat{\beta_0}$, 
   $\hat{\beta_1}$ a použijeme jako koeficienty přímky.
3. Dokreslíme průnik v počátku.

  4. Model vyhodnotíme pomocí funkce `summary()`

```{r}
summary(md1)
```

Souhrn obsahuje původní zadání v podobě rovnice, dále identifikované koeficienty modelu, významnost závislosti indikuje přítomnost jedné nebo více `*` u vysvětlující proměnné; dále lze vyčíst podíl vysvětlené variabiltiy `Adjusted R-squared:  0.6417`.\

```{r}
par(mfrow = c(2, 2))
plot(md1, 1:4)
```

R dále poskytuje grafické nástroje k posouzení vhodnosti zvoleného modelu. Zde nás budou zajímat zejména **residua** modelu. Dle prvního grafu by mohl model se závislostí na polynomu být lepší volbou.\

Pokusme se sestavit alternativní model s kvadratickou vysvětlující proměnnou.

```{r}
md2 <- lm(y ~ poly(x, 2))
summary(md2)
```

```{r}
par(mfrow = c(2, 2))
plot(md2, 1:4)
```

```{r}
with(data = dfr, 
     expr = plot(x, y, xlim = c(0, 30)))
pred <- predict(md2)
ix <- sort(dfr$x, index.return=T)$ix
lines(x[ix], pred[ix], col = 'orangered')
```

Srovnejme modely vzájemně

```{r}
AIC(md1, md2)
```

::: callout-tip
## Cvičení

  1. Prozkoumejte závislost odtoku na velikosti povodí\ 

| Povodí | velikost $x_i$ | pořadí | odtok $y_i$ | pořadí | rozdíl $d_i$ | $d_i^2$ |      
|--------|----------------|--------|-------------|--------|--------------|---------|     
| 1      |                |        |             |        |              |         |     
| 2      |                |        |             |        |              |         |      
| 3      |                |        |             |        |              |         |      
| 4      |                |        |             |        |              |         |      
| 5      |                |        |             |        |              |         |     
| 6      |                |        |             |        |              |         |     
| 7      |                |        |             |        |              |         |     
| 8      |                |        |             |        |              |         |     
| 9      |                |        |             |        |              |         |     
| 10     |                |        |             |        |              |         |     
  
:::
